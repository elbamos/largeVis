% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hdbscan.R
\name{hdbscan}
\alias{hdbscan}
\title{hdbscan}
\usage{
hdbscan(
  neighbors = NULL,
  minPts = 20,
  K = 5,
  verbose = getOption("verbose", TRUE)
)
}
\arguments{
\item{neighbors}{The output of either \code{\link{randomProjectionTreeSearch}} or \code{\link{largeVis}}.}

\item{minPts}{The minimum number of points in a cluster.}

\item{K}{The number of points in the core neighborhood. (See details.)}

\item{verbose}{Verbosity.}
}
\value{
An object of type \code{hdbscan} with the following fields:
\describe{
   \item{'clusters'}{A vector of the cluster membership for each vertex. Outliers
   are given \code{NA}}
   \item{'probabilities'}{A vector of the degree of each vertex' membership. This
   is calculated by standardizing each vertex' \eqn{lambda_p} against the \eqn{lambda_birth}
   and \eqn{lambda_death} of the cluster.}
   \item{'glosh'}{A vector of GLOSH outlier scores for each node assigned to a cluster. NA for nodes not
   in a cluster.}
   \item{'tree'}{The minimum spanning tree used to generate the clustering.}
   \item{'hierarchy'}{A representation of the condensed cluster hierarchy.}
   \item{'call'}{The call.}
 }

 The hierarchy describes the complete post-condensation structure of the tree:
 \describe{
 \item{'nodemembership'}{The cluster ID of the vertex's immediate parent, after condensation.}
 \item{'lambda'}{\eqn{\lambda_p} for each vertex.}
 \item{'parent'}{The cluster ID of each cluster's parent.}
 \item{'stability'}{The cluster's stability, taking into account child-node stabilities.}
 \item{'selected'}{Whether the cluster was selected.}
 \item{'coredistances'}{The core distance determined for each vertex.}
 \item{'lamba_birth'}{\eqn{\lambda_b} for each cluster.}
 \item{'lambda_deaeth'}{\eqn{\lambda_d} for each cluster.}
 }
}
\description{
Implemenation of the hdbscan algorithm.
}
\details{
The hyperparameter \code{K} controls the size of core neighborhoods.
The algorithm measures the density around a point as 1 / the distance between
that point and its \eqn{k}th nearest neighbor. A low value of \code{K} is similar
to clustering nearest neighbors rather than based on density. A high value of
\code{K} may cause the algorithm to miss some (usually contrived) clustering
patterns, such as where clusters are made up of points arranged in lines to form
shapes.

The function must be provided sufficient nearest-neighbor data for whatever
is specified for \eqn{k}. If \eqn{k} = 5, for example, the edge matrix (and
neighbor matrix, if specified) must contain data on at least 5 neighbors for
each point. This should not be problematic in typical use in connection with
\code{\link{largeVis}}, which is ordinarily run with a far higher \eqn{k}-value
than hdbscan.
}
\note{
This is not precisely the \code{HDBSCAN} algorithm because it relies on the
nearest neighbor data generated by the \code{LargeVis} algorithm. In particular,
\code{HDBSCAN} assumes that all points can be connected in a single minimal-spanning
tree. This implementation uses a minimal-spanning forest, because the graph may not
be fully connected depending on the amount of nearest-neighbor data provided.
If, for example, the data has distinct clusters where no member of some cluster is a
nearest neighbor of a point in any other cluster, which can easily happen, the algorithm will
generate distinct trees for each disconnected set of points. In testing, this
improved the performance of the algorithm.

Do not rely on the content of the \code{probabilities} field for outliers. A future version
will hopefully provide some measure of outlier factor.
}
\examples{
\dontrun{
library(largeVis)
library(clusteringdatasets)  # See https://github.com/elbamos/clusteringdatasets
data(spiral)
dat <- as.matrix(spiral[, 1:2])
neighbors <- randomProjectionTreeSearch(t(dat), K = 10, max_iter = 5)
edges <- buildEdgeMatrix(t(dat), neighbors)
clusters <- hdbscan(edges, neighbors = neighbors, verbose = FALSE)

# Calling largeVis while setting sgd_batches to 1 is
# the simplest way to generate the data structures neeeded for hdbscan
spiralVis <- largeVis(t(dat), K = 10, max_iter = 5, sgd_batches = 1)
clusters <- hdbscan(spiralVis, verbose = FALSE)
# The gplot function helps to visualize the clustering
largeHighDimensionalDataset <- matrix(rnorm(50000), ncol = 50)
vis <- largeVis(largeHighDimensionalDataset)
clustering <- hdbscan(vis)
gplot(clustering, t(vis$coords))
}
}
\references{
R. Campello, D. Moulavi, and J. Sander, Density-Based Clustering Based on Hierarchical Density Estimates In: Advances in Knowledge Discovery and Data Mining, Springer, pp 160-172. 2013
}
\seealso{
\url{https://github.com/lmcinnes/hdbscan}
}
