---
title: 'largeVis: An Implementation of the LargeVis Algorithm'
author: "Amos Elberg"
date: '`r Sys.Date()`'
output:
  rmarkdown::html_vignette:
    toc: true
    fig_caption: yes
bibliography: largevisvignettes.bib
params:
  mnist_path: /Volumes/Datasets2/DATASETS/mnist/raw
  graph_file: /Volumes/Datasets2/DATASETS/YouTubeCommunities/com-youtube.ungraph.txt
  communities: /Volumes/Datasets2/DATASETS/YouTubeCommunities/com-youtube.top5000.cmty.txt
vignette: |
  %\VignetteIndexEntry{largeVis: An Implementation of the LargeVis Algorithm}    
  %\VignetteEngine{knitr::rmarkdown}    
  %\VignetteEncoding{UTF-8}
---

```{r setupvignette,eval=T,echo=F,warning=F,error=F,message=F}
require(ggplot2, 
        quietly = TRUE)
knitr::opts_chunk$set(collapse = TRUE, 
                      comment = "#>",
                      cache=FALSE)

theme_set(
  theme_bw() %+replace%
  theme(
    legend.title = element_text(size = rel(0.8),
                              face = "bold"),
    legend.margin = margin(0.1,0.1,0.1,0.1, "cm"),
    legend.position = "bottom",
    legend.key.size = unit(0.5, "lines"),
    legend.text = element_text(size = unit(8, "points")), 
    axis.title.y = element_text(angle = 90),
    axis.text = element_text(size = rel(0.7)),
    plot.margin = margin(0, 0.5, 1, 0, "lines"), 
    axis.title = element_text(size = rel(0.8),
                              face = "bold"),
    title = element_text(size = rel(0.9))
  ) 
)

require(largeVis,quietly = TRUE)

if (
	file.exists(params$mnist_path)
	&& require(deepnet) 
	&& require(tidyr)
	&& require(magrittr)
	&& require(dplyr)
) .build_mnist <- TRUE else .build_mnist <- FALSE

if (
	file.exists(params$graph_file) 
	&& file.exists(params$communities)
) .build_youtube <- TRUE else .build_youtube <- FALSE
```
This Vingette provides an overview of the largeVis package.  

## Introduction

This package provides `LargeVis` visualizations, fast nearest-neighbor search, and density-based clustering algorithms that take advantage of the data structures created by the nearest neighbor search.  Primarily, it provides the `LargeVis` algorithm, presented in @tang2016visualizing, which creates high-quality low-dimensional representaitons of large, high-dimensional datasets, similar to [t-SNE](https://lvdmaaten.github.io/tsne/).  

These visualizations are useful for data exploration, for visualizing complex non-linear functions, and especially for visualizing embeddings such as learned vectors for images. 

A limitation of t-SNE is that because the algorithm has complexity order $O(n^2)$, it is not feasible for use on even moderately sized datasets.  [Barnes-Hut](https://arxiv.org/pdf/1301.3342.pdf), an approximation of t-SNE, has complexity $O(n \log n)$ but also quickly becomes infeasible as the size of data grows. `LargeVis` is intended to address the issue by operating in linear $O(n)$ time.  It has been benchmarked at more than 30x faster than Barnes-Hut on datasets of approximately 1-million rows, and scaled linearly as long as there is sufficient RAM. 

In addition, `LargeVis` includes an algorithm for finding approximate k-Nearest Neighbors in $O(n)$ time. This algorithm turns out to be faster at finding accurate a-NNs than any other method I was able to test. 

The package also includes a function for visualizing image embeddings by plotting images at the locations given by the `LargeVis` algorithm.

For a detailed description of the algorithm, please see the original paper, @tang2016visualizing.

In addition, the package provides efficient implementations of the `DBSCAN`, `OPTICS`, and `HDBSCAN` density-based clustering algorithms, which have been optimized to take advantage of the data structures created to implement `LargeVis`.  

## Package Overview

### LargeVis Implementation

The `LargeVis` portion of the package offers five functions for visualizing high-dimensional datasets and finding approximate nearest neighbors (along with some helper functions):

1.  `randomProjectionTreeSearch`, a method for finding approximate nearest neighbors.
2.  `projectKNNs`, which takes as input a weighted nearest-neighbor graph and estimates a projection into a low-dimensional space.
3.  `largeVis`, which implements the entire `LargeVis` algorithm.
4.  `manifoldMap` (and companon `ggManifoldMap`), which produce a plot for visualizing embeddings of images. 
5. `buildWijMatrix` takes a sparse matrix of the distances between nearest neighbors, and returns an $\omega$ matrix, as described in the paper, with edges properly weighted for use in `projectKNNs`.

See the [original paper](https://arxiv.org/abs/1602.00370) for a detailed description of the algorithm. 

Beyond the original paper, this implementation supports using momentum training, a technique from deep learning, which can dramatically speed-up convergence of the algorithm. 

### Density-Based Clustering

`largeVis` also includes three clustering algorithms:  `DBSCAN`, `OPTICS`, and `HDBSCAN`. 

The implementations here are not intended for general use. Rather, the `LargeVis` algorithm calculates, and `largeVis` retains, nearest neighbor information for inputs. Much of the computation time of the three clustering algorithms is spent identifying neighbors and calculating distances. The intent of these implementations is, by resuing the neighbor data generated by `largeVis`, to make it practical to apply these computationally expensive algorithms to very large datasets. 

The clustering algorithms are provided by the following functions:

1.  `lv_dbscan`, implements the `DBSCAN` [@Ester96adensity-based] algorithm, which attempts to achieve a flat clustering using a consistent density threshold. 
2. 	`lv_optics`, implements the `OPTICS` [@Ankerst:1999:OOP:304181.304187] algorithn, which applies similar logic as `DBSCAN` to build a hierarchical clustering.  
3.  `hdbscan`, implements the `HDBSCAN` [@Campello2013] algorithm, which aims to make the `DBBSCAN` and `OPTICS` approach flexible for graphs with different densities in different regions, eliminating the need to select certain hyperparameters. 

## largeVis

### Preparing Data For largeVis

For input to `largeVis`, data should be scaled, NA's, Infs and NULL removed, and transposed from the R-standard so that examples are columns and features are rows. Duplicates should be removed as well.

If there are NA's, Infs, or NULLs in the input, `randomProjectionTreeSearch` will definitely fail. 

If the numerical range covered by the data is large, this can cause errors in or before the `buildWijMatrix` function. This is because the algorithm requires calculating $\exp(||\vec{x_i}, \vec{x_j}||^2)$ in the high-dimensional space, which will overflow if the distance between any nearest neighbors exceeds about 26. The `largeVis` function will attempt to detect this situation and automatically scale the data if necessary. 

Duplicates in the input data could cause issues in earlier versions of this package. These issues should be resolved in later versions. If you experience problems with duplicate points, please contact the package author.  

### the `randomProjectionTreeSearch` Function

This function uses a two-phase algorithm to find approximate nearest neighbors. In the first phase, which is [Erik Bernhardsson](http://erikbern.com)'s [Annoy](https://github.com/spotify/annoy) algorithm, `n_trees` trees are formed by recursively dividing the space by hyperplanes until the number of nodes in the branch is lower than the dimensionality of the dataset.  A node's candidate nearest neighbors are the union of all nodes with which it shared a leaf on any of the trees.  The `LargeVis` algorithm adds a second phase, neighborhood exploration, which considers, for each node, whether the candidate neighbors of the node's candidate immediate neighbors are closer. The logic of the algorithm is that a node's neighbors' neighbors are likely to be the node's own neighbors. In each iteration, the closest `K` candidate neighbors for each node are kept. 

The authors of @tang2016visualizing suggest that a single iteration of the second phase is generally sufficient to obtain satisfactory performance. My own benchmarks confirm this. Adding a single iteration of exploration improves the accuracy of the nearest neighbor search more efficiently than increasing the number of ANNOY trees. The utility of additional iterations declines as the number of iterations increases.

### The `projectKNNs` Function

This function takes as its input a `Matrix::sparseMatrix`, of connections between nodes. The matrix must be symmetric. A non-zero cell implies that node `i` is a nearest neighbor of node `j`, vice-versa, or both. Non-zero values represent the strength of the connection relative to other nearest neighbors of the two nodes. 

The `LargeVis` algorithm, explained in detail in @tang2016visualizing, estimates the embedding by sampling from the identified nearest-neighbor connections. For each edge, the algorithm also samples `M` non-nearest neighbor negative samples. `M`, along with $\gamma$ and $\alpha$, control the visualization. $\alpha$ controls the desired distance between nearest neighbors. $\gamma$ controls the relative strength of the attractive force between nearest neighbors and repulsive force between non-neighbors.

The effect of varying these hyperparameters is explored below.

### The `largeVis` Function

The `largeVis` function combines `randomProjectionTreeSearch` and `projectKNNs`, along with additional logic for calculating edge weights, to implement the complete `LargeVis` algorithm. 

### The Alpha and Gamma Hyperparameters

The following grid illustrates the effect of the $\alpha$ and $\gamma$ hyperparameters on the `mnist` dataset:

```{r alphagamma,echo=T,fig.width=6,fig.height=6,warning=F}
library(largeVis)
if (.build_mnist) {
	mnist <- deepnet::load.mnist(params$mnist_path)
	data <- t(mnist$train$x)
	labels <- mnist$train$y
	visObj <- largeVis(data, sgd_batches = 1)
	
	inputs <- tidyr::expand_grid(gamma = c(5,7,14), alpha = c(0.1, 1, 10))
	
	agcoords <- purrr::map2(inputs$alpha, inputs$gamma, function(a, g) {
		obj <- projectKNNs(visObj$wij, dim = 2, alpha = a, gamma = g)
		data.frame(t(obj)) %>%
			set_colnames(c("x", "y")) %>%
			mutate(alpha = a, gamma = g)
	}) %>% bind_rows()
	
	agcoords$label = rep(labels, nrow(inputs))
	
	ggplot(agcoords,
	       aes(x = x, 
	           y = y, 
	           color = factor(label))) +
	  geom_point(alpha = 0.1, 
	             size = 0.1) +
	  facet_grid(alpha ~ gamma,
	             labeller = label_bquote(alpha == .(alpha), 
	                                     gamma == .(gamma)),
	             scales = 'free') +
	  scale_x_continuous(breaks = NULL, 
	                     name = "") +
	  scale_y_continuous(breaks = NULL, 
	                     name = "") +
	  ggtitle(expression(paste("Effect of ", alpha, " vs. ", gamma, sep = "  "))) +
	  guides(color = FALSE)
}
```


### Negative Sampling 

During the stochastic gradient descent phase of `largeVis`, \code{M} nodes that do not have edges to the current point are sampled along with each edge. In the original paper, the negative nodes are selected by sampling weighted according to their degree raised to the $\frac{3}{4}$ power. In the reference implementation, however, the nodes are weighted according to $P_n(j) \propto (\sum_{i \iff j \in N_i} w_{ij})^{0.75}$. 

Versions of `largeVis` prior to 0.1.8 (i.e., before the reference implementation became available) used degree. These results may be more aesthetically pleasing to many people. Later versions therefore allow this to be set as a parameter. The default is to use edge weights, as in the reference implementation. 

The difference is that using edge weights, the resulting clusters tend to be more plainly convex. Using degree encourages a wider variation of irregular shapes. The effect is imperceptible at small data sizes; is subtly noticeable at around 50,000 nodes; but becomes pronounced on datasets of greater than 1 million nodes.  

The following chart demonstrates the difference using the `mnist` dataset. 

```{r usedegree,echo=T,fig.width=6,fig.height=4,warning=F}
if (.build_mnist) {
	mnist <- deepnet::load.mnist(params$mnist_path)
	data <- t(mnist$train$x)
	labels <- mnist$train$y
	preVis <- largeVis(data, K = 100, sgd_batches = 1)
	
	noDegree <- projectKNNs(preVis$wij)
	degree <- projectKNNs(preVis$wij, useDegree = TRUE)
	degreeCoords <- data.frame(rbind(t(noDegree), t(degree)),
														 label = factor(c(labels, labels)),
														 degree = factor(rep(c(
														 	"weights", "degree"
														 ),
														 each = ncol(degree))))
	
	ggplot(degreeCoords, aes(x = X1, y = X2, color = label)) +
		geom_point(size = 0.02, alpha = 0.1) +
		facet_grid(. ~ degree) +
		scale_x_continuous("", labels = NULL, breaks = NULL) +
		scale_y_continuous("", labels = NULL, breaks = NULL) +
		guides(color = FALSE) +
		ggtitle("Effect of useDegree")
}
```

### The M and K Hyperparameters

The following chart illustrates the effect of the `M` hyperparameter from `projectKNNs` and `K`, the number of nearest neighbors, again using the `mnist` dataset.

```{r m_and_k,echo=T,fig.width=6,fig.height=6,warning=F}
if (.build_mnist) {
	Ks <- c(75, 150, 300)
	Ms <- c(1, 5, 10)
	
	coord_list <- c()
	
	for (k in Ks) {
		visObj <- largeVis(data, K = k, sgd_batches = 1)
		for (m in Ms) {
			coords <- projectKNNs(visObj$wij, M = m)
			new_data <- data.frame(t(coords)) %>%
				set_colnames(c("x", "y")) %>%
				mutate(label = labels, M = m, K = k)
			coord_list <- c(coord_list, list(new_data))
		}
	}
	
	kmcoords <- bind_rows(coord_list)
	
	ggplot(kmcoords, aes(x = x, y = y, color = factor(label))) + 
		geom_point(size = 0.1, alpha = 0.1) + 
		guides(color = F) +
	  scale_x_continuous("", 
	                     breaks = NULL) +
	  scale_y_continuous("", 
	                     breaks = NULL) +
	  facet_grid(K ~ M, 
	             scales = 'free', 
	             labeller = label_bquote(K == .(K), M == .(M))) +
	  ggtitle("Effect of M and K")
}
```

### Momentum Training 

Momentum training is a method for optimizing stochastic gradient descent to encourage more rapid convergence.  The technique is common in deep learning.

Momentum alters the gradient so that in each iteration the algorithm calculates the update for each parameter taking into account prior updates of the same parameter. If the gradient for a parameter has the same direction on one iteration as the previous, momentum will cause it to move further in that direction. If the gradient reverses, the update will be smaller than it would be otherwise. 

Without momentum, each iteration updates the low dimensional embedding according to the equation $\theta_{t + 1} = \theta_t + \rho\frac{dO}{d\theta_t}$

With momentum, this becomes:  $\theta_{t + 1} = \theta_t + \mu_t$ where $\mu_t = \rho\frac{dO}{d\theta_t} + \lambda\mu_{t - 1}$

The momentum paramter, $\lambda$, controls the rate of decay. 

Adding momentum can make it possible to reduce the number of sgd batches, in some cases substantially, without reducing the visual quality of the result:

```{r momentum,echo=T,fig.width=6,fig.height=6,warning=F}
if (.build_mnist) {
	starterCoords <-
		t(matrix(runif(n = 2 * ncol(mnist)) - 0.5, ncol = 2))
	firstCoords <- data.frame(
		t(
			projectKNNs(preVis$wij, coords = starterCoords, sgd_batches = 0.1)
		),
		lambda = 0,
		batches = 0.1,
		label = labels
	)
	for (batches in c(0.3, 0.8)) {
		newCoords <- data.frame(scale(t(
			projectKNNs(
				preVis$wij,
				verbose = TRUE,
				sgd_batches = batches,
				coords = t(starterCoords)
			)
		)))
		newCoords$lambda <- 0
		newCoords$batches <- batches
		newCoords$label <- labels
		firstCoords <<- rbind(firstCoords, newCoords)
	}
	for (lambda in c(0.4, 0.9)) {
		for (batches in c(0.1, 0.3, 0.5)) {
			newCoords <- data.frame(scale(t(
				projectKNNs(
					preVis$wij,
					verbose = TRUE,
					sgd_batches = batches,
					momentum = lambda,
					coords = t(starterCoords)
				)
			)))
			newCoords$lambda <- lambda
			newCoords$batches <- batches
			newCoords$label <- labels
			firstCoords <<- rbind(firstCoords, newCoords)
		}
	}
	momentumCoords <- firstCoords
	momentumCoords$label <- factor(momentumCoords$label)
	
	ggplot(momentumCoords, aes(x = X1, y = X2, color = label)) +
		geom_point(size = 0.01, alpha = 0.1) +
		facet_grid(
			batches ~ lambda,
			scales = "free",
			labeller = label_bquote(cols = lambda == .(lambda),
															rows = b == .(batches))
		) +
		scale_x_continuous("",
											 limits = c(-40, 40),
											 labels = NULL,
											 breaks = NULL) +
		scale_y_continuous("",
											 limits = c(-40, 40),
											 labels = NULL,
											 breaks = NULL) +
		guides(color = FALSE) +
		ggtitle("Effect of Momentum and Reduced Training Batches")
}
```

### `manifoldMap`

The `manifoldMap` function is useful when the examples being clustered are themselves images. Given a coordinate matrix (as generated by `projectKNNs` or `vis`) and an `array` of `N` images, the function samples `n` images and plots them at the coordinates given in the matrix. 

```{r manifold,echo=T,fig.width=6,fig.height=6,warning=F}
if (.build_mnist) {
	mnistCoords <- largeVis(data, K = 150)$coords
	images <- mnist$train$x
	dim(images) <- c(60000, 28, 28)
	images <- aperm(images, perm = c(1,3,2), resize = FALSE)
	manifoldMap(t(mnistCoords),
	    n = 1000,
	    scale = 0.1,
	    images = images,
	    xlab = "", 
	    ylab = "")
}
```

### Support for Sparse Matrices

`largeVis` supports sparse matrices.  Besides facilitating very large datasets, this makes it practicable to visualize term-document-matrices directly, and compare the result with the result of visualizing topic vectors. 

### Visualizing Graphs

The `largeVis` visualization algorithm can be used to visualize graphs, which may be weighted, and directed or undirected. This can be done in one of three ways: By visualizing the graph directly, by calculating and visualizing the $\omega_{ij}$ matrix (see @tang2016visualizing for details), or by calculating the distance between nodes. 

The following code illustrates how to import and visualize a directed, unweighted graph directly and by using a $\omega_{ij}$ matrix, using the YouTube-communities dataset available [here](https://snap.stanford.edu/data/com-Youtube.html). 

```{r youtube,echo=T,fig.width=7,fig.height=14}
if (.build_youtube) {
	library(Matrix)
	pathToGraphFile <- params$graph_file 
	pathToCommunities <- params$communities
	youtube <- readr::read_tsv(pathToGraphFile, skip=4, col_names=FALSE)
	youtube <- as.matrix(youtube)
	youtube <- Matrix::sparseMatrix(i = youtube[, 1],
	                                j = youtube[, 2],
	                                x = rep(1, nrow(youtube)), 
	                                dims = c(max(youtube), max(youtube)))
	
	communities <- readr::read_lines(pathToCommunities)
	communities <- lapply(communities, 
	                      FUN = function(x) as.numeric(unlist(strsplit(x, "\t"))))
	community_assignments <- rep(NA, 
	                             nrow(youtube))
	for (i in 1:length(communities)) community_assignments[communities[[i]]] <- i
	
	wij <- buildWijMatrix(youtube)
	wij_coordinates <- projectKNNs(wij)
	
	# The input matrix for projectKNNs must be symmetricsized
	youtube <- youtube + t(youtube)
	direct_coordinates <- projectKNNs(youtube)
	
	youTube_coordinates <- data.frame(rbind(t(wij_coordinates), t(direct_coordinates)))
	colnames(youTube_coordinates) <- c("x", "y")
	youTube_coordinates$community <- factor(rep(community_assignments, 2))
	youTube_coordinates$set <- rep(c("wij", "direct"), each = ncol(wij_coordinates))

	youTube_coordinates %>% 
		mutate(
			alpha = if_else(is.na(community), .001, .1),
			size = if_else(is.na(community), .001, .01),
			x = x / sd(x), 
			y = y / sd(y)
		) %>%
		arrange(if_else(is.na(community), 0, 1)) %>%
	ggplot(aes( x = x, 
	                      y = y, 
	                      color = community, 
							size = size,
							alpha = alpha
	                      )) +
	  geom_point() +
		scale_size_binned(range = c(0.0001, 0.1)) +
		scale_alpha_binned(range = c(0.0001, .1)) +
	  scale_x_continuous("", breaks = NULL, limits = c(-2.2, 2.2)) +
	  scale_y_continuous("", breaks = NULL, limits = c(-2.2, 2.2)) +
	  ggtitle("YouTube Communities") +
		facet_grid(set ~ .) +
		coord_equal() +
	  guides(color = FALSE, size = FALSE, alpha = FALSE)
}
```

### Distance Methods

The original `LargeVis` paper used Euclidean distances exclusively.  The `largeVis` package offers a choice among Euclidean, Cosine, Manhattan, and Hamming distance measures.  

### Memory Consumption

The algorithm is necessarily memory-intensive for large datasets. 

A simple way to reduce peak memory usage, is to turn-off the `save_neighbors` parameter when running `vis`. If this is insufficient, the steps of the algorithm can be run separately with the `neighborsToVectors`, `distance`, and `buildEdgeMatrix` functions.  In this case, the workflow is:

```{r lowmemexample,eval=F,echo=T}
neighbors <- randomProjectionTreeSearch(largeDataset)
neighbors$neighbors <- NULL
gc()
wij <- buildWijMatrix(neighbors$edgematrix)
rm(neighbors)
gc()
coords <- projectKNNs(wij)
```

Note that `gc()` is being called explicitly. The reason is that R will not collect garbage while executing the package's C++ functions, which can require substantial temporary RAM. 

Memory requirements during the neighbor search may be managed by reducing `n_trees` or building the ANNOY index on-disk. The decrease in precision from reducing the number of trees is marginal, and may be compensated-for by increasing `max_iters`. 

## Clustering

`largeVis` also includes three clustering algorithms:  `DBSCAN`, `OPTICS`, and `HDBSCAN`. 

The implementations here are not intended for general use. Rather, the `LargeVis` algorithm calculates, and `largeVis` retains, nearest neighbor information for inputs. Much of the computation time of the three clustering algorithms is spent identifying neighbors and calculating distances. The intent of these implementations is, by resuing the neighbor data generated by `largeVis`, to make it practical to apply these very computationally expensive algorithms to very large datasets. 

### Overview of Density Based Clustering

All three algorithms attempt to cluster points by linking a point to its nearest neighbors, using a modified definition of distance.  The `core distance` of a point is defined as the distance to its Kth nearest neighbor. It is an (inverse) measure of the density of space around a point. The `reachability distance` between two points is the greater of the distance between them and `core distances`.  When the neighbors of a point, $p$, are identified to link it with its nearest neighbors, no point $q$ can be closer to $p$ than $p$'s `core distance`. Thus, `reachability distance` combines both distance and density.  (Differences in the definition of `core distance` and `reachability distance` among the algorithms are not addressed here.)  

### DBSCAN

`DBSCAN` [@Ester96adensity-based] attempts to achieve a flat clustering using a consistent density threshold, $\epsilon$.  Points with more than `minPts` neighbors in their $\epsilon$-neighborhoods are `core points`.  Where the $\epsilon$ neighborhoods of corepoints overlap, the neighborhoods are merged into a single cluster. `DBSCAN` thus takes two hyper-parameters, $\epsilon$ and `minPts`. 

The objects returned by the `largeVis` `DBSCAN` implementation are compatible with the `dbscan` objects producted by package `dbscan`. [@hahsler]

The following chart illustrates the effect of the $\epsilon$ and `minPts` parameters on the performance of `DBSCAN`.

```{r dbscan,fig.width=6,fig.height=6}
load(system.file(package = "largeVis", "vignettedata/spiral.Rda"))
dat <- spiral
vis <- largeVis(t(dat), K = 20, save_edges = TRUE, save_neighbors = TRUE, sgd_batches = 1)
set <- rbind(Map(f = function(y) {
	rbind(Map(f = function(x) {
		clust = lv_dbscan(vis, eps = x, minPts = y)$cluster
		data.frame(cluster = clust, eps = x, minPts = y)
	}, c(1, 3, 5)))
}, c(5, 10, 20)))
lbind <- function(x) do.call(rbind, x)
set <- lapply(set, FUN = lbind)
set <- lbind(set)
set$x <- rep(dat[, 1], 9)
set$y <- rep(dat[, 2], 9)
set$cluster <- factor(set$cluster)
set$eps <- ordered(set$eps, labels = paste("epsilon == ", c(1, 3, 5), sep = ""))
set$minPts <- ordered(set$minPts, labels = paste("minPts == ", c(5, 10, 20), sep = ""))
ggplot(data = set, aes(x = x, y = y, color = cluster)) +
	geom_point(size = 0.5, alpha = 0.7) +
	facet_grid(minPts ~ eps, labeller = label_parsed) + 
	scale_x_continuous("", breaks = NULL) +
	scale_y_continuous("", breaks = NULL) +
	guides(color = FALSE) +
	ggtitle("Effect of eps and minPts on DBSCAN results")
```

### OPTICS

`OPTICS` [@Ankerst:1999:OOP:304181.304187] applies similar logic as `DBSCAN` to build a hierarchical clustering. Beginning with a seed point, `OPTICS` finds the point with the lowest reachability distance (up to the cutoff, $\epsilon$) to any point already in the graph, and adds it. The result is an ordering of points by decreasing density. When there are no points with a `reachability distance` to the graph less than $\epsilon$, `OPTICS` begins again with a new seed point.  

This implementation of `OPTICS` has a hyperparameter `useQueue` which controls the order in which points are processed. In effect, this controls what points will be the first point in newly discovered clusters. The reachability graph above used `useQueue = FALSE`, which is used by the `dbscan` package and the `ELKI` implementation. If `useQueue` is set to `TRUE`, then the algorithm will use 
points in denser regions of the space as the seeds for new clusters.

This is illustrated in the following `reachability plots` for the spiral dataset:

```{r optics,fig.width=5,message=FALSE,warning=FALSE}
optClust <- lv_optics(vis, eps = 5, useQueue = FALSE, minPts = 5)
optClust2 <- lv_optics(vis, eps = 5, useQueue = TRUE, minPts = 5)
ggplot(data.frame(
	o = c(optClust$order, optClust2$order), 
	d = c(optClust$reachdist, optClust2$reachdist), 
	useQueue = rep(c("No Queue", "Use Queue"), each = length(optClust$order))
), aes(x = o, y = d)) + 
		geom_point(stat = 'identity', size = 0.1) + 
		geom_bar(stat = 'identity', alpha = 0.3) +
		facet_grid(useQueue ~ .) +
		scale_x_continuous("Order") + 
		scale_y_continuous("Reachability Distance")
```

`OPTICS` can be thought of as a hierarchical generalization of `DBSCAN`. `OPTICS` clusterings are readily convertible to `DBSCAN` clusterings by cutting clusters when the `reachability distance` exceeds a threshold. This can be done using the `extractDBSCAN` function of the `dbscan` package. The resulting clusters are identical to what would be obtained using `DBSCAN` with the same parameters, except in assignment of noise points. (This is not shown, to avoid creating a dependency on the `dbscan` package.) 

```{r opticsvsdbscan,fig.width=2,fig.width=6,eval=F,echo=T}
suppressWarnings(opticsPoints <- do.call(rbind, Map(f = function(x) {
		clust = thiscut <- dbscan::extractDBSCAN(optClust, x)$cluster
		data.frame(cluster = clust, eps = x)
	}, c(1, 3, 5))))
opticsPoints$cluster <- factor(opticsPoints$cluster)
opticsPoints$x <- rep(dat[, 1], 3)
opticsPoints$y <- rep(dat[, 2], 3)
opticsPoints$eps <- factor(paste("epsilon ==" , opticsPoints$eps, sep = ""))

ggplot(data = opticsPoints, aes(x = x, y = y, color = cluster)) +
	geom_point(size = 0.5, alpha = 0.7) +
	facet_grid(. ~ eps, labeller = label_parsed) + 
	scale_x_continuous("", breaks = NULL) +
	scale_y_continuous("", breaks = NULL) +
	guides(color = FALSE) +
	ggtitle("OPTICS Clusters With EPS Cuts")
```

The `dbscan` package has other functions for cutting and visualizing `OPTICS` clusterings as well. The `optics` objects produced by `largeVis` are compatible with the ones produced by the `dbscan` package.

### HDBSCAN

`HDBSCAN` [@Campello2013] aims to make the `DBBSCAN` and `OPTICS` approach flexible for graphs with different densities in different regions, by building a hierarchical density-bsaed clustering from which flat clusters are extracted.  `HDBSCAN` does not need the $\epsilon$ hypterparameter, which can be difficult to set. Rather, `HDBSCAN`, in effect, determines different values for $\epsilon$ for different parts of the dataset. 

```{r hdbscan,fig.width=6,fig.height=6}
hdbscan_set <- expand_grid(minPts = c(6, 10, 20), K = c(2, 6, 12))

set <- purrr::map2(hdbscan_set$minPts, hdbscan_set$K, function(x, y) {
	hdclust <- largeVis::hdbscan(vis, K = y, minPts = x)$cluster
	data.frame(cluster = as.numeric(hdclust), K = y, minPts = x)
}) %>%
	bind_rows()

set$x <- rep(dat[, 1], 9)
set$y <- rep(dat[, 2], 9)
set$cluster <- factor(set$cluster)
set$K <- factor(paste("K=", set$K))
set$minPts <- factor(paste("minPts=", set$minPts))

ggplot(data = set, aes(x = x, y = y, color = cluster)) +
	geom_point(size = 0.5, alpha = 0.7) +
	facet_grid(minPts ~ K) + 
	scale_x_continuous("", breaks = NULL) +
	scale_y_continuous("", breaks = NULL) +
	guides(color = FALSE) +
	ggtitle("HDBSCAN Is Robust\nTo Hyperparameter Changes")
```

The `largeVis` implementation of `HDBSCAN` produces flat and hierarchical clusterings simultaneously. 

The `HDBSCAN` clusterings can be visualized with the `gplot` function:

```{r gplot,fig.width=6,fig.height=6,warning=F}
hdclust <- largeVis::hdbscan(vis, K = 2 , minPts = 20)
gplot(hdclust, spiral)
```



## References
