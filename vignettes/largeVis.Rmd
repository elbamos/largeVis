---
title: 'largeVis: An Implementation of the LargeVis Algorithm'
author: "Amos Elberg"
date: '`r Sys.Date()`'
output:
  rmarkdown::html_vignette:
    fig_caption: yes
bibliography: TangLZM16.bib
vignette: >
  %\VignetteIndexEntry{largeVis}   
  %\VignetteEngine{knitr::rmarkdown}   
  %\VignetteEncoding{UTF-8}
---

```{r setup,eval=T,echo=F,warning=F,error=F,message=F}
# Note to reader:  Please don't steal the semi-distinctive visual style I spent several minutes creating for myself.
require(ggplot2, 
        quietly = TRUE)
require(RColorBrewer, 
        quietly = TRUE)
require(wesanderson, 
        quietly = TRUE)
knitr::opts_chunk$set(collapse = TRUE, 
                      comment = "#>")
colors_discrete <- function(x) rep(wes_palette("Darjeeling", 
                                               n = min(x, 5)), 
                                   2)[1:x]
colors_divergent_discrete <- function(x) 
  grDevices::colorRampPalette(RColorBrewer::brewer.pal(x, "Spectral"))
colors_continuous <-  function(x) wes_palette(name = "Zissou",
                                              n = x, 
                                              type = "continuous")

nacol <- colors_discrete(4)[4]
theme_set(
  theme_bw() %+replace%
  theme(
    legend.key.size = unit(4, "mm"), 
    legend.title = element_text(size = rel(0.8),
                              face = "bold"),
    legend.margin = unit(0, "cm"),
    legend.position = "bottom",
    legend.key.size = unit(0.5, "lines"),
    legend.text=element_text(size = unit(8, "points")), 
    axis.title.y = element_text(angle = 90),
    axis.text = element_text(size = rel(0.7)),
    plot.margin = unit(c(0, 0.5, 1, 0), "lines"), 
    axis.title = element_text(size = rel(0.8),
                              face = "bold"),
    title = element_text(size = rel(0.9))
  ) 
)
rebuild <- FALSE
if (!exists("buildManifolds")) buildManifolds <- rebuild

require(largeVis,quietly = TRUE)
```
This Vingette provides an overview of the largeVis package.  

## Introduction

The `largeVis` package offers four functions for visualizing high-dimensional datasets and finding approximate nearest neighbors, based on the `LargeVis` algorithm presented in @tang2016visualizing:

1.  `randomProjectionTreeSearch`, a method for finding approximate nearest neighbors.
2.  `projectKNNs`, which takes as input a weighted nearest-neighbor graph and estimates a projection into a low-dimensional space.
3.  `vis`, which combines `randomProjectionTreeSearch`, `buildEdgeMatrix`, and `projectKNNs`, along with additional code to implement the `LargeVis` algorithm.
4.  `manifoldMap`, which produces a plot for visualizing embeddings of images. 

See the [original paper](https://arxiv.org/abs/1602.00370) for a detailed description of the algorithm. 

## Data Preparation

For input to `largeVis`, data should be scaled, NA's, Infs and NULL removed, and transposed from the R-standard so that examples are columns and features are rows. Duplicates should be removed as well.

If there are NA's, Infs, or NULLs in the input, `randomProjectionTreeSearch` will definitely fail. 

If the numerical range covered by the data is large, this can cause errors in or before the `buildEdgeMatrix` function. This is because the algorithm requires calculating $\exp(||\vec{x_i}, \vec{x_j}||^2)$ in the high-dimensional space, which will overflow if the distance between any nearest neighbors exceeds about 26.  

If there are duplicates in the input data, while the implementation tries to filter duplicates, it is likely to lead to problems. If the number of duplicates is large, this can cause the random projection tree search to fail. If the number is small, the algorithm may identify a sufficient number of neighbors, but an error may then occur during `buildEdgeMatrix`, or stochastic gradient descent. 

## Examples

```{r MNIST,echo=F,message=F,warning=F,results='hide',eval=rebuild}
darch::provideMNIST(download=T)
load("data/train.RData")

mnistCoords <- vis(t(trainData) - 0.5, 
                   K = 40, 
                   tree_threshold = 700, 
                   n_trees = 40, 
                   max_iter = 2, 
                   verbose=F)
mnistCoords <- mnistCoords$coords
mnistCoords <- scale(t(mnistCoords))
mnistCoords <- data.frame(mnistCoords)
colnames(mnistCoords) <- c("x", "y")
labs <- apply(trainLabels, 
              MARGIN = 1, 
              FUN = function(x) which(x == 1))
mnistCoords$labels <- factor(labs - 1)
```

```{r drawmnist,echo=F,warning=F}
load(system.file("extdata", "mnistcoords.Rda", package="largeVis"))
ggplot(mnistCoords, aes(x = x, y = y, color = labels)) +
  geom_point(size = 0.1, alpha = 0.3) +
  scale_x_continuous(name = "", limits = c(-2, 2), breaks = NULL) +
  scale_y_continuous(name = "", limits = c(-2, 2), breaks = NULL) +
  scale_color_manual(values = colors_divergent_discrete(10)(10)) +
  guides(colour = guide_legend(override.aes = list(size=5))) +
  ggtitle("MNIST")
```

```{r ldafromldavis,echo=F,eval=rebuild}
library(LDAvis)
data("TwentyNewsgroups")
theta <- scale(t(TwentyNewsgroups$theta))
set.seed(1974)
initialcoords <- matrix(rnorm(2 * ncol(theta)), ncol = ncol(theta))
visObj <- vis(theta, 
              K = 100, 
              n_trees = 20, 
              tree_threshold = 100, 
              max_iter = 2)

ngcoords <- scale(t(visObj$coords))
ngcoords <- data.frame(ngcoords)
colnames(ngcoords) <- c("x", "y")
library(lda)
data("newsgroup.train.labels")
ngcoords$label <- factor(newsgroup.train.labels)[-1]
```
```{r draw20ng,echo=F,warning=FALSE,error=FALSE,message=FALSE}
load(system.file("extdata", "ngcoords.Rda", package = "largeVis"))
ggplot(ngcoords, 
       aes(x = x, 
           y = y, 
           color = label)) +
  geom_point(size = 0.4, alpha = 0.5) + 
  scale_color_manual(values = colors_divergent_discrete(20)(20),
                     guide=FALSE) +
  scale_x_continuous(name = "", limits = c(-2, 2.5), breaks = NULL) +
  scale_y_continuous(name = "", limits = c(-2, 2.5), breaks = NULL) +
  ggtitle("20 Newsgroups")
```

```{r wikiterms,eval=rebuild,echo=F}
# # The data file must be obtained directly from the paper authors
# wikiwords <- readLINE("/mnt/hfsshare/DATASETS/Wiki_embedding/wiki_word_vec_100d.txt")
# wikiwords[is.nan(wikiwords)] <- 0
```

## Overview of Functions and Hyperparameters

### `randomProjectionTreeSearch`

This function uses a two-phase algorithm to find approximate nearest neighbors. In the first phase, which is based on [Erik Bernhardsson](http://erikbern.com)'s [Annoy](https://github.com/spotify/annoy) algorithm, `n_trees` trees are formed by recursively dividing the space by hyperplanes until at most `tree_threshold` nodes remain in a branch.  A node's candidate nearest neighbors are the union of all nodes with which it shared a leaf on any of the trees.  The `largeVis` algorithm adds a second phase, neighborhood exploration, which considers, for each node, whether the candidate neighbors of the node's candidate immediate neighbors are closer. The logic of the algorithm is that a node's neighbors' neighbors are likely to be the node's own neighbors. In each iteration, the closest `K` candidate neighbors for each node are kept. 

(Note that this implementation of `largeVis` differs from the approach taken by `Annoy`, in that `Annoy` always uses the number of features as the leaf threshold, where `largeVis` allows this to be an adjustable parameter.)

The authors of @tang2016visualizing suggest that a single iteration of the second phase is generally sufficient to obtain satisfactory performance. 

The chart below illlustrates the trade-off between performance and accuracy for the nearest-neighbor search, using various hyperparameters.  The data was produced using the `benchmark.R` script in the `inst/` directory.  The test data is the 1-million vector, 128-feature [SIFT Dataset](http://corpus-texmex.irisa.fr/), as per Erik Bernhardsson's [ANN Benchmark](https://github.com/erikbern/ann-benchmarks) github. 

```{r performance,echo=F,eval=rebuild}
benchmark <- readr::read_csv(system.file("extdata", "results.csv", package="largeVis"), 
                             col_names = FALSE)

colnames(benchmark) <- c("time", 
                       "precision", 
                       "n_trees", 
                       "max_iters", 
                       "threshold",
                       "method", 
                       "tree_type", 
                       "search_type", 
                       "eps")
benchmark$series <- factor(paste(benchmark$method, 
                                 benchmark$max_iters))
benchmark$labels <- paste(benchmark$n_trees, ", ", 
                         benchmark$threshold, sep = "")
benchmark$labels[grepl("Annoy", 
                      benchmark$series)] <-
  benchmark$n_trees[grepl("Annoy", 
                          benchmark$series)]
benchmark$labels[benchmark$max_iters > 1] <- ""
```
```{r plotpeformance,echo=F,fig.width=5,fig.height=6,fig.align='center',warning=FALSE,message=FALSE}
load(system.file("extdata", "benchmark.Rda", package = "largeVis"))
ggplot(benchmark, aes(x = time, 
                      y = precision / 100, 
                      group = series, 
                      color = series, 
                      shape = series,
                      label = labels)) +
  geom_point(size = 1) + 
  geom_line(size = 0.5) + 
  geom_text(size = 3, 
            nudge_x = 10) +
  scale_x_continuous(name = expression(
    atop("Time",
         atop(italic("Annoy labels show n_trees, largeVis labels show n_trees,threshold"))
         )
    )) + 
  scale_y_log10("Precision", 
                limits = c(0.1,1), 
                breaks = c(.1, .25, .5, .8, .9, .99)) +
  scale_color_manual(values =      colors_divergent_discrete(nlevels(benchmark$series))(nlevels(benchmark$series))) +
  guides(color = guide_legend(nrow=3)) +
  ggtitle(expression(
    atop("Time vs. Precision (K = 100, n = 10000)",
         atop(italic("(Upper Left is Better"))
         )
    ))
```

The `largeVis` series are labelled by the number of neighbor-exploration iterations. 

The difference between `RcppAnnoy` and `RcppAnnoy-Full` is that `Annoy` is designed for the construction of a static tree that can then be queried, while `largeVis` finds nearest neighbors for all nodes at the same time. The times shown for `RcppAnnoy` are the times to fetch neighbors only for the 10000 rows that were used to test. `RcppAnnoy-full`, like `largeVis`, shows the time to fetch neighbors for the entire dataset. 

The data confirms the recommendation of the paper authors' concerning the number of iterations of neighborhood exploration: While the first iteration offers a substantial benefit, however it is more efficient to improve accuracy by increase the number of trees or size of the tree threshold than by adding iterations. 

If `randomProjectionTreeSearch` fails to find the desired number of neighbors, usually the best result is obtained by increasing the tree threshold. If `randomProjectionTreeSearch` fails with an error that no neighbors were found for some nodes, and the tree threshold is already reasonable, this may be an indication that duplicates remain in the input data. 

### `projectKNNs`

This function takes as its input a `Matrix::sparseMatrix`, of connections between nodes. The matrix must be symmetric. A non-zero cell implies that node `i` is a nearest neighbor of node `j`, vice-versa, or both. Non-zero values represent the strength of the connection relative to other nearest neighbors of the two nodes. 

The `LargeVis` algorithm, explained in detail in @tang2016visualizing, estimates the embedding by sampling from the identitied nearest-neighbor connections. For each edge, the algorithm also samples `M` non-nearest neighbor negative samples. `M`, along with $\gamma$ and $\alpha$, control the visualization. $\alpha$ controls the desired distance between nearest neighbors. $\gamma$ controls the relative strength of the attractive force between nearest neighbors and repulsive force between non-neighbors.

The following grid illustrates the effect of the $\alpha$ and $\gamma$ hyperparameters, using the `wiki` dataset which is included with the package:

```{r wikihyperparameters,echo=F,eval=rebuild}
data(wiki)

inputs <- data.frame(
  g = rep(c(.5,1,7,14), 4),
  a = rep(c(.1,1,5,10), each = 4)
)
set.seed(1974)
initialcoords <- matrix(rnorm(ncol(wiki) * 2), nrow = 2)

agcoords <- do.call(rbind, 
                    lapply(1:nrow(inputs), 
                           FUN = function(x) {
  a <- inputs[x, 'a']
  g <- inputs[x, 'g']
  newcoords <- initialcoords
  localcoords <- projectKNNs(wiki, 
                             alpha =  a, 
                             gamma = g,
                             verbose = FALSE, 
                             coords = newcoords)
  localcoords <- data.frame(scale(t(localcoords)))
  colnames(localcoords) <- c("x", "y")
  localcoords$a <- a
  localcoords$g <- g
  localcoords$activity <- log(Matrix::colSums(wiki))
  localcoords  
}))
```
```{r drawhyperparameters,echo=F,fig.width=3.5,fig.height=4,fig.align='center'}
load(system.file("extdata", "agcoords.Rda", package="largeVis"))
ggplot(agcoords,
       aes(x = x, 
           y = y, 
           color = activity)) +
  geom_point(alpha = 0.2, 
             size = 0.05) +
  facet_grid(a ~ g,
             labeller = label_bquote(alpha == .(a), 
                                     gamma == .(g)),
             scales = 'free') +
  scale_x_continuous(breaks = NULL, 
                     name = "") +
  scale_y_continuous(breaks = NULL, 
                     name = "") +
  scale_color_gradientn(colors = colors_continuous(10), 
                        guide=FALSE) +
  ggtitle(expression(paste("Effect of ", alpha, " vs. ", gamma, sep = "  ")))
```

The additional hyperparameters $\rho$ and `min-`$\rho$ control the starting and final learning rate for the stochastic gradient descent process. 

The algorithm can treat positive edge weights in two different ways. The authors of @tang2016visualizing suggest that edge weights should be used to generate a weighted sampling.  However, the algorithm for taking a weighted sample runs in $O(n \log n)$.  Alternatively, the edge-weights can be applied to the gradients.  This is controlled by the `weight_pos_samples` parameter. 

### `vis`

The `vis` function combines `randomProjectionTreeSearch` and `projectKNNs`, along with additional logic for calculating edge weights, to implement the complete `LargeVis` algorithm. 

The following chart illustrates the effect of the `M` and `K` parameters, using the `iris` dataset. Each row re-uses the same set of identified `K` neighbors, and initial coordinates. 

```{r iris,echo=F,fig.width=5,fig.height=5,eval=rebuild}
data(iris)
Ks <- c(5, 10, 20, 30)
Ms <- c(1, 5, 10, 20)
data(iris)
dat <- iris[,1:4]
dupes <- duplicated(dat)
dat <- dat[-dupes,]
labels <- iris$Species[-dupes]
dat <- scale(dat)
dat <- as.matrix(dat)
dat <- t(dat)

set.seed(1974)
coordsinput <- matrix(rnorm(ncol(dat) * 2), nrow = 2)

iriscoords <- do.call(rbind, lapply(Ks, FUN = function(K) {
  neighbors <- randomProjectionTreeSearch(dat, 
                                          K = K, 
                                          verbose = FALSE)
  neighborIndices <- neighborsToVectors(neighbors)
  distances <- largeVis::distance(x = dat, 
                                  neighborIndices$i, 
                                  neighborIndices$j)
  wij <- buildEdgeMatrix(i = neighborIndices$i, 
                       j = neighborIndices$j, 
                       d = distances)
  do.call(rbind, lapply(Ms, FUN = function(M) {
    coords <- projectKNNs(wij = wij$wij, M = M, coords = coordsinput)
    coords <- scale(t(coords))
    coords <- data.frame(coords)
    colnames(coords) <- c("x", "y")
    coords$K <- K
    coords$M <- M
    coords$Species <- as.integer(labels)
    coords
  }))
}))
  
iriscoords$Species <- factor(iriscoords$Species)
levels(iriscoords$Species) <- levels(iris$Species)
```
```{r drawiriscoords,echo=F,fig.width=4,fig.height=4.5,fig.align='center'}
load(system.file("extdata", "iriscoords.Rda", package="largeVis"))
ggplot(iriscoords,
       aes(x = x,
           y = y,
           color = Species)) +
         geom_point(size = 0.5) +
  scale_x_continuous("", 
                     breaks = NULL) +
  scale_y_continuous("", 
                     breaks = NULL) +
  facet_grid(K ~ M, 
             scales = 'free', 
             labeller = label_bquote(K == .(K), M == .(M))) +
  scale_color_manual(values = colors_discrete(3)) +
  ggtitle("Effect of M and K on Iris Dataset")
```

### `manifoldMap`

The `manifoldMap` function is useful when the examples being clustered are themselves images. Given a coordinate matrix (as generated by `projectKNNs` or `vis`) and an `array` of `N` images, the function samples `n` images and plots them at the coordinates given in the matrix. If the `transparency` parameter is a number between 0 and 1, then the function adds to each image an alpha channel where the value per pixel is proportional to $transparency *$ the image content. 

The function can plot both color and greyscale images. 

#### Example with MNIST Letters

The following code will plot 5000 images sampled from the MNIST dataset at positions generated by `vis`:
```{r loadmnistimages,eval=rebuild,echo=F}
load("data/train.RData")
```
```{r drawmanifoldmap,echo=T,fig.width=8,fig.height=8,message=F,warning=F,fig.align='center'}
par(mai=c(0.25, 0.25, 0.25, 0.25))
if (exists("trainData")) {
  dim(trainData) <- c(60000, 28, 28)
  set.seed(1974)
  manifoldMap(mnistCoords[,1:2],
      n = 5000,
      scale = 0.003,
      transparency = F,
      images = trainData,
      xlab = "", 
      ylab = "",
      xlim = c(-2, 2),
      ylim = c(-2, 2))
} 
```

The code is disabled by default in this vignette for data size reasons.

#### Example with Faces

The following examples visualize facial-recognition embedding vectors from the [Labelled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) dataset.  The embedding vectors were graciously provided by [Brandon Amos](https://bamos.github.io/) of the [OpenFace](https://cmusatyalab.github.io/openface/) project. Similar vectors may be generated for images using the OpenFace `batch-represent` command.

```{r lfw,echo=F,eval=rebuild}
data("facevectors")

faceembeddings <- t(as.matrix(facevectors[,-c(1:2)]))
faceVis <- vis(faceembeddings, K = 50, 
               max_iter = 2, 
               n_trees = 100, 
               distance_method = 'Cosine')

faceCoords <- data.frame(scale(t(faceVis$coords)))
colnames(faceCoords) <- c("x", "y")
faceCoords <- cbind(faceCoords, facevectors[,1:2])
```

OpenFace embedding vectors encode an image in such a way that the embeddings for multiple images of the same person should be similar.  This is illustrated on the first plot below, which highlights the locations of the embedding vectors for images of 10 selected individuals.  (The 10 were selected on the basis that there are a large number of images of each in the dataset, and the author would reognize them.)

```{r plotFaceVectors,echo=F,fig.width=5,fig.height=5}
load(system.file("extdata", "faces.Rda", package="largeVis"))
faceCoCopy <- faceCoords
lvs <- c("Tony_Bennette", 
         "Gloria_Gaynor", 
         "Jennifer_Aniston", 
         "Kobe_Bryant", 
         "John_Nash",
         "Jack_Smith", 
         "Nancy_Kerrigan", 
         "Nora_Ephron",
         "Julianna_Margulies", 
         "Abdullah_al-Attiyah")
faceCoCopy$name[! faceCoCopy$name %in% lvs] <- "Other"
faceCoCopy$name <- factor(faceCoCopy$name)
faceCoCopy$alpha <- factor(ifelse(faceCoCopy$name == "Other", 0.05, 0.2))

ggplot(faceCoCopy, aes(x = x, 
                       y = y, 
                       color = name,
                       alpha = alpha,
                       size = alpha)) + 
  geom_point() +
  scale_alpha_manual(values = c(0.2, 0.8), guide = FALSE) + 
  scale_color_manual(values = c("grey", colors_divergent_discrete(10)(10))) +
  scale_size_manual(values = c(0.2, 0.5), guide = FALSE) +
  scale_x_continuous("", 
                     breaks = NULL) +
  scale_y_continuous("", 
                     breaks = NULL) +
  ggtitle("Positions of Embedding Vectors for Selected Individuals")
```

The function of `manifoldMap` is illustrated in the following plot, which places 300 images from the dataset at the locations given by the `largeVis` map.  

The plot is disabled by default because it requires obtaining the face images directly from [Labelled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/).

```{r faceImages,eval=buildManifolds,echo=F,fig.width=8,fig.height=8}
load(system.file("extdata", "faceLabels.Rda", package="largeVis"))
library(jpeg)
set.seed(1974)
n <- 500
facesToPlot <- sample(nrow(faceCoords), n)

faces <- apply(faceLabels[facesToPlot,], MARGIN = 1, FUN = function(x) {
  ret <- readJPEG(paste("/mnt/hfsshare/DATASETS/lfw faces/lfw",
                x[1], sub("png", "jpg", x[2]), sep = "/"))
  dim(ret) <- c(250, 250, 3)
  ret
})
dim(faces) <- c(250, 250, 3, n)
faces <- aperm(faces, c(4,1,2,3)) # n,h,w,c
par(bg = 'grey5', mai=c(0.25, 0.25, 0.25, 0.25))
manifoldMap(x = faceCoords[facesToPlot,1:2], 
            n = n, images = 1 - faces, scale =  1 / 1000,
            xlab = NULL, ylab = NULL, col.lab = 'gray5',
            col.axis = 'gray5')
```

## Support for Sparse Matrices

`largeVis` supports sparse matrices.  Besides facilitating very large datasets, this makes it practicable to visualize term-document-matrices. 

For example, the following plot visualizes a corpus of 5000 political blog entries, as included with the `stm` package.  The first row visualizes the blog entries as tf-idf weighted term vectors; the second, as topic vectors.

```{r tdm,echo=F,eval=rebuild}
library(stm)
data("poliblog5k")
p <- c(0, cumsum(as.numeric(lapply(poliblog5k.docs, function(x) ncol(x)))))
i <- do.call("c", lapply(poliblog5k.docs, function(x) x[1,]))
p[length(p)] <- length(i)
j <- rep(0:(length(diff(p)) - 1), diff(p))
v <- do.call("c", lapply(poliblog5k.docs, function(x) x[2,]))
poli <- Matrix::sparseMatrix(i = i + 1, 
                             j = j + 1, 
                             x = v)
dupes <- duplicated(slam::as.simple_triplet_matrix(Matrix::t(poli)))
poli <- poli[, ! dupes]
poli <- poli / log(Matrix::rowSums(poli > 0)) # tf-idf weight
set.seed(1974)
inputcoords <- matrix(rnorm(ncol(poli) * 2 ), nrow = 2)
policoords <- vis(poli, 
                  K = 100, 
                  n_trees = 20, 
                  tree_threshold = 100, 
                  max_iter = 10,
                  M = 10,
                  gamma = 15,
                  distance_method = 'Cosine',
                  verbose = FALSE)
polidata <- data.frame(scale(t(policoords$coords)))
colnames(polidata) <- c('x', 'y')
polidata$rating <- poliblog5k.meta$rating[!dupes]
polidata$blog <- poliblog5k.meta$blog[!dupes]
```

```{r stm,echo=F,eval=rebuild}
library(stm)
data(poliblog5k)
set.seed(1974)
stmmodel <- stm(poliblog5k.docs, poliblog5k.voc, K = 20, 
                data = poliblog5k.meta, prevalence = ~ rating + s(day),
                content = ~rating,
                max.em.its = 75, init.type="Spectral", seed = 1974)

stmvectors <- t(scale(stmmodel$theta))

stmVis <- vis(stmvectors, K = 100, tree_threshold = 100, 
    max_iter = 10, 
    n_trees = 100)

stmdata <- data.frame(scale(t(stmVis$coords)))
colnames(stmdata) <- c('x', 'y')
stmdata$rating <- poliblog5k.meta$rating
stmdata$blog <- poliblog5k.meta$blog
```

```{r drawtdm,echo=F,fig.height=4,fig.width=7}
load(system.file("extdata", "polidata.Rda", package="largeVis"))
load(system.file("extdata", "stm.Rda", package="largeVis"))
polidata$origin <- "tf-idf weighted term vectors"
stmdata$origin <- "stm topic vectors"

polidata$x <- scale(polidata$x)
polidata$y <- scale(polidata$y)
stmdata$x <- scale(stmdata$x)
stmdata$y <- scale(stmdata$y)

combined <- rbind(polidata, 
                  stmdata)
combined$origin <- factor(combined$origin)
combined$origin <- factor(combined$origin, 
                          levels = rev(levels(combined$origin)))

ggplot(combined, aes(x = x, 
                     y = y, 
                     color = blog)) +
  geom_point(size = 0.2, 
             alpha = 0.8) +
  scale_color_manual(values = colors_divergent_discrete(6)(6)) +
  facet_grid(origin ~ rating, 
             scale = 'free') +
  scale_x_continuous("", 
                     breaks = NULL, 
                     limits = c(-2.5, 2.5)) +
  scale_y_continuous("", 
                     breaks = NULL,
                     limits = c(-2.5, 2.5)) +
  ggtitle("5000 Political Blog Entries")
```

This facilitates evaluation of the effectiveness of a topic model.

## Visualizing Graphs

The `largeVis` visualization algorithm can be used to visualize ordinary graphs.  The included `wiki` dataset is an example.

The following code illustrates how to import and visualize a graph using the YouTube-communities dataset available [here](https://snap.stanford.edu/data/com-Youtube.html). 

```{r youtube,eval=F,echo=T}
pathToGraphFile <- 
  "/mnt/hfsshare/DATASETS/YouTubeCommunities/com-youtube.ungraph.txt"
pathToCommunities <- 
  "/mnt/hfsshare/DATASETS/YouTubeCommunities/com-youtube.top5000.cmty.txt"

youtube <- readr::read_tsv(pathToGraphFile, skip=4, col_names=FALSE)
youtube <- as.matrix(youtube)
youtube <- Matrix::sparseMatrix(i = youtube[, 1],
                                j = youtube[, 2],
                                x = rep(1, nrow(youtube)), 
                                dims = c(max(youtube), max(youtube)))
youtube <- youtube + t(youtube)
communities <- readr::read_lines(pathToCommunities)
communities <- lapply(communities, 
                      FUN = function(x) as.numeric(unlist(strsplit(x, "\t"))))
community_assignments <- rep(0, 
                             nrow(youtube))
for (i in 1:length(communities)) community_assignments[communities[[i]]] <- i

youTube_coordinates <- projectKNNs(youtube)
youTube_coordinates <- data.frame(scale(t(youTube_coordinates)))
colnames(youTube_coordinates) <- c("x", "y")
youTube_coordinates$community <- factor(community_assignments)
youTube_coordinates$alpha <- factor(ifelse(youTube_coordinates$community == 0, 0.05, 0.2))
```

```{r drawYouTube,echo=F,warning=FALSE,message=FALSE}
load(system.file("extdata", "youtube.Rda", package="largeVis"))
ggplot(youTube_coordinates, aes( x = x, 
                      y = y, 
                      color = community, 
                      alpha = alpha, 
                      size = alpha)) +
  geom_point() +
  scale_color_manual(values = 
                       c("black", colors_continuous(5000)),
                     guide = FALSE) +
  scale_alpha_manual(values = c(0.005, 0.2), guide = FALSE) +
  scale_size_manual(values = c(0.03, 0.15), guide = FALSE) +
  scale_x_continuous("", 
                     breaks = NULL, limits = c(-2.5,2.5)) +
  scale_y_continuous("", 
                     breaks = NULL, limits = c(-2.5,2.5)) +
  ggtitle("YouTube Communities")
```

```{r livejournal,eval=F,echo=T}
pathToGraphFile <- 
  "/mnt/hfsshare/DATASETS/livejournal/com-lj.ungraph.txt"
pathToCommunities <- 
  "/mnt/hfsshare/DATASETS/livejournal/com-lj.top5000.cmty.txt"

livejournal <- readr::read_tsv(pathToGraphFile, skip=4, col_names=FALSE)
livejournal <- as.matrix(livejournal) + 1
livejournal <- Matrix::sparseMatrix(i = livejournal[, 1],
                                j = livejournal[, 2],
                                x = rep(1, nrow(livejournal)), 
                                dims = c(max(livejournal), max(livejournal)))
livejournal <- livejournal + t(livejournal)
communities <- readr::read_lines(pathToCommunities)
communities <- lapply(communities, 
                      FUN = function(x) as.numeric(unlist(strsplit(x, "\t"))) + 1)
community_assignments <- rep(0, 
                             nrow(livejournal))
for (i in 1:length(communities)) community_assignments[communities[[i]]] <- i

todelete <- which(rowSums(livejournal) == 0)
livejournal <- livejournal[- todelete, - todelete]
community_assignments <- community_assignments[- todelete]

livejournal_coordinates <- projectKNNs(livejournal)
livejournalyouTube_coordinates <- data.frame(scale(t(youTube_coordinates)))
colnames(livejournal_coordinates) <- c("x", "y")
livejournal_coordinates$community <- factor(community_assignments)
livejournal_coordinates$alpha <- factor(ifelse(livejournal_coordinates$community == 0, 0.05, 0.2))
```

```{r drawLJ,echo=F,warning=FALSE,message=FALSE,eval=F}
load(system.file("extdata", "livejournal.Rda", package="largeVis"))
ggplot(livejournal_coordinates, aes( x = x, 
                      y = y, 
                      color = community, 
                      alpha = alpha, 
                      size = alpha)) +
  geom_point() +
  scale_color_manual(values = 
                       c("black", colors_continuous(5000)),
                     guide = FALSE) +
  scale_alpha_manual(values = c(0.005, 0.2), guide = FALSE) +
  scale_size_manual(values = c(0.03, 0.15), guide = FALSE) +
  scale_x_continuous("", 
                     breaks = NULL, limits = c(-2.5,2.5)) +
  scale_y_continuous("", 
                     breaks = NULL, limits = c(-2.5,2.5)) +
  ggtitle("LiveJournal Communities")
```

## Distance Methods

The original `LargeVis` paper used Euclidean distances exclusively.  The `largeVis` package offers a choice among Euclidean and Cosine distance measures.  

## Memory Consumption

The algorithm is necessarily memory-intensive for large datasets. `neighborsToVectors`, `distance`, and `buildEdgeMatrix` are available as separate functions to facilitate memory-efficient handling of large datasets, because the high-dimensional dataset is not needed after distances have been calculated. In this case, the workflow is:

```{r eval=rebuild,echo=T}
neighbors <- randomProjectionTreeSearch(largeDataset)
neighborIndices <- neighborsToVectors(neighbors)
rm(neighbors)
distances <- distance(neighborIndices$i, 
                      neighborIndices$j,
                      largeDataset)
rm(largeDataset)
wij <- buildEdgeMatrix(i = neighborIndices$i, 
                       j = neighborIndices$j, 
                       d = distances)
rm(distances, neighborIndices)
coords <- projectKNNs(wij$wij)
```

In testing, this method reduced peak RAM requirements by more than 70%. 

## Bibliography
